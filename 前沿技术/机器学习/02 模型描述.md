- 监督学习中, 我们提前有一组数据, 这组数据被称为训练集

**符号相关**
- m : 训练样本数量
- x : 输入特征
- y : 输出变量 (也称目标变量)
- (x ,y) : 一个训练样本
- $(x^{(i)} , y^{(i)})$ : 表示第i个训练样本

**监督学习算法工作流程**
- 输入训练集
- 通过学习算法
- 输出一个函数f (function, 以前称为hypothesis, 假设函数)
	- 该函数的作用是对一个给定的输入x进行预测, 预测出来的数据被称为$\hat{y}$
	- function被称为model
	- x被称为特征
	- $\hat y$被称为预测
		- y是给出的训练集x对应的真实值, 而$\hat y$给定的是预测值

### 线性回归
- linear regression

**如何决定假设函数f**
- $f_{w,b}(x)=w x+b$ , 常缩写为$f(x)$
- 这种模型被称为线性回归(Linear regression)
	- 对于公式$f(x)=w x+b$ , 为一元线性回归
	- 也被称为单变量线性回归

**为什么使用线性函数**
- 更简单

### 成本函数

- 成本函数告诉我们模型的运行情况以让我们做的更好
- 对于函数$f(x)=w x+b$
	- 其中的$w$与$b$被称为模型参数, 有时也被称为系数或权重
	- 机器学习中可以通过在训练期间调整模型参数来改进模型
	- 选择的模型参数在对应的x时应尽量接近y
- 线性回归要解决的是一个最小化问题
	- $f(x^i) - y^i$ 的值的大小被称为误差 (error)
	- 要找到最小化的误差, 即尽量减小 $\sum_{i=1}^{m} { ( f(x^i) - y^i )^2 }$
	- 为构建一个不会随着训练集增大而不断增大的成本函数, 通常会选择将累加结果除2m
		- 即 $Cost Function=J(w,b)=\frac{1}{2m}\sum_{i=1}^{m} { ( f(x^i) - y^i )^2 }$
	- 这又被称为**平方误差成本函数**
- 不同的人会针对不同的应用程序使用不同的成本函数, 但是平方误差成本函数是迄今为止线性回归最常用的函数
- 线性回归的目标是找到w或w,b , 使成本函数J的值最小
- 模型 : $\underset{w,b}{minimize} J(w,b)$

- 当同时有多个参数w,b时, 其成本函数图像类似下图
![[前沿技术/机器学习/Inbox/Pasted image 20250426171816.png]]



